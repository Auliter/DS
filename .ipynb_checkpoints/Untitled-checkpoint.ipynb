{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb0792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>re</th>\n",
       "      <th>income</th>\n",
       "      <th>tx</th>\n",
       "      <th>dx</th>\n",
       "      <th>wt</th>\n",
       "      <th>ht</th>\n",
       "      <th>bmi</th>\n",
       "      <th>leg</th>\n",
       "      <th>arml</th>\n",
       "      <th>armc</th>\n",
       "      <th>waist</th>\n",
       "      <th>tri</th>\n",
       "      <th>sub</th>\n",
       "      <th>gh</th>\n",
       "      <th>albumin</th>\n",
       "      <th>bun</th>\n",
       "      <th>SCr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51624</td>\n",
       "      <td>male</td>\n",
       "      <td>34.166667</td>\n",
       "      <td>Non-Hispanic White</td>\n",
       "      <td>[25000,35000)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87.4</td>\n",
       "      <td>164.7</td>\n",
       "      <td>32.22</td>\n",
       "      <td>41.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>100.4</td>\n",
       "      <td>16.4</td>\n",
       "      <td>24.9</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51626</td>\n",
       "      <td>male</td>\n",
       "      <td>16.833333</td>\n",
       "      <td>Non-Hispanic Black</td>\n",
       "      <td>[45000,55000)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.3</td>\n",
       "      <td>181.3</td>\n",
       "      <td>22.00</td>\n",
       "      <td>42.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>26.6</td>\n",
       "      <td>74.7</td>\n",
       "      <td>10.2</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51628</td>\n",
       "      <td>female</td>\n",
       "      <td>60.166667</td>\n",
       "      <td>Non-Hispanic Black</td>\n",
       "      <td>[10000,15000)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>116.8</td>\n",
       "      <td>166.0</td>\n",
       "      <td>42.39</td>\n",
       "      <td>35.3</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.2</td>\n",
       "      <td>118.2</td>\n",
       "      <td>29.6</td>\n",
       "      <td>35.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51629</td>\n",
       "      <td>male</td>\n",
       "      <td>26.083333</td>\n",
       "      <td>Mexican American</td>\n",
       "      <td>[25000,35000)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>97.6</td>\n",
       "      <td>173.0</td>\n",
       "      <td>32.61</td>\n",
       "      <td>41.7</td>\n",
       "      <td>38.7</td>\n",
       "      <td>37.0</td>\n",
       "      <td>103.7</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51630</td>\n",
       "      <td>female</td>\n",
       "      <td>49.666667</td>\n",
       "      <td>Non-Hispanic White</td>\n",
       "      <td>[35000,45000)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.7</td>\n",
       "      <td>168.4</td>\n",
       "      <td>30.57</td>\n",
       "      <td>37.5</td>\n",
       "      <td>36.1</td>\n",
       "      <td>33.3</td>\n",
       "      <td>107.8</td>\n",
       "      <td>30.3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seqn     sex        age                  re         income  tx  dx     wt  \\\n",
       "0  51624    male  34.166667  Non-Hispanic White  [25000,35000)   0   0   87.4   \n",
       "1  51626    male  16.833333  Non-Hispanic Black  [45000,55000)   0   0   72.3   \n",
       "2  51628  female  60.166667  Non-Hispanic Black  [10000,15000)   1   1  116.8   \n",
       "3  51629    male  26.083333    Mexican American  [25000,35000)   0   0   97.6   \n",
       "4  51630  female  49.666667  Non-Hispanic White  [35000,45000)   0   0   86.7   \n",
       "\n",
       "      ht    bmi   leg  arml  armc  waist   tri   sub   gh  albumin   bun   SCr  \n",
       "0  164.7  32.22  41.5  40.0  36.4  100.4  16.4  24.9  5.2      4.8   6.0  0.94  \n",
       "1  181.3  22.00  42.0  39.5  26.6   74.7  10.2  10.5  5.7      4.6   9.0  0.89  \n",
       "2  166.0  42.39  35.3  39.0  42.2  118.2  29.6  35.6  6.0      3.9  10.0  1.11  \n",
       "3  173.0  32.61  41.7  38.7  37.0  103.7  19.0  23.2  5.1      4.2   8.0  0.80  \n",
       "4  168.4  30.57  37.5  36.1  33.3  107.8  30.3  28.0  5.3      4.3  13.0  0.79  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as  pd\n",
    "df = pd.read_stata('nhgh.dta')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292c5751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       seqn     sex        age                  re         income  tx  dx  \\\n",
      "9     51643  female  43.000000  Non-Hispanic Black  [35000,45000)   1   1   \n",
      "10    51645    male  66.416667    Mexican American   [5000,10000)   0   0   \n",
      "13    51651  female  19.333333  Non-Hispanic Black  [20000,25000)   0   0   \n",
      "20    51660    male  32.833333    Mexican American  [25000,35000)   0   0   \n",
      "30    51672  female  27.916667  Non-Hispanic White  [45000,55000)   0   0   \n",
      "...     ...     ...        ...                 ...            ...  ..  ..   \n",
      "6761  62109  female  17.750000  Non-Hispanic Black   [5000,10000)   0   0   \n",
      "6763  62112  female  58.333333      Other Hispanic  [10000,15000)   1   1   \n",
      "6776  62136    male  70.333333    Mexican American   [5000,10000)   0   0   \n",
      "6790  62155    male  33.000000    Mexican American  [35000,45000)   0   0   \n",
      "6794  62160  female  63.583333      Other Hispanic            NaN   1   1   \n",
      "\n",
      "         wt     ht    bmi   leg  arml  armc  waist   tri   sub    gh  albumin  \\\n",
      "9     107.7  164.3  39.90  32.7  36.5  39.6  129.8  27.0   NaN  11.0      3.6   \n",
      "10     82.9  171.3  28.25   NaN   NaN   NaN    NaN   NaN   NaN   5.7      4.4   \n",
      "13    111.6  165.7  40.65  38.5  35.6  40.5  117.4   NaN   NaN   6.2      4.1   \n",
      "20     85.7  159.5  33.69  35.0  37.0  32.6  105.3  20.6   NaN   6.0      4.4   \n",
      "30    105.8  149.7  47.21   NaN   NaN   NaN    NaN   NaN   NaN   5.2      4.0   \n",
      "...     ...    ...    ...   ...   ...   ...    ...   ...   ...   ...      ...   \n",
      "6761  104.8  168.5  36.91  42.0  37.0  42.5  119.5   NaN   NaN   9.5      4.2   \n",
      "6763   53.7  147.9  24.55  34.3  32.2  26.2   84.7  23.2   NaN   6.3      4.4   \n",
      "6776   90.5  166.0  32.84   NaN   NaN   NaN    NaN   NaN   NaN   5.7      3.7   \n",
      "6790   94.3  163.5  35.28  34.4  34.7  35.5  112.3  20.2   NaN   5.4      4.1   \n",
      "6794   71.3  157.3  28.82  31.4  33.0  30.9  102.1  20.1  19.7   6.7      4.3   \n",
      "\n",
      "       bun   SCr  \n",
      "9     16.0  2.54  \n",
      "10     9.0  0.86  \n",
      "13     9.0  0.71  \n",
      "20     8.0  0.72  \n",
      "30    10.0  0.60  \n",
      "...    ...   ...  \n",
      "6761   6.0  0.58  \n",
      "6763  15.0  0.70  \n",
      "6776   8.0  0.78  \n",
      "6790  10.0  0.97  \n",
      "6794  15.0  0.63  \n",
      "\n",
      "[1452 rows x 20 columns]\n",
      "6795\n",
      "1452\n",
      "971\n"
     ]
    }
   ],
   "source": [
    "dfNa = df[df.isna().any(axis=1)]\n",
    "dfNa2 = df[df['sub'].isna()]\n",
    "print(dfNa)\n",
    "print(len(df.index))\n",
    "print(len(dfNa.index))\n",
    "print(len(dfNa2.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f52fbe2",
   "metadata": {},
   "source": [
    "<font color='blue' ><b>\n",
    "From the above result, it can be seen that there is about 1/4 of the data containing NA values. By a close examination of the rows, it can be seen that many of them are missing values in columns of subscapular skinfold thickness measurement. According to the result from Andrea, Rodrigo, Et Al the value of subscapular skinfold thickness has a very strong association with the developing of type2 diabetes (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6960014/) which lead to a decisio to drop these rows instead of filling them with an zero/mean value.\n",
    "<b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9fe887b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def labelRace(re):\n",
    "    if re == \"Other Hispanic\" or re == \"Mexican American\":\n",
    "        return 0.25\n",
    "    elif re == \"Non-Hispanic Black\":\n",
    "        return 0.5\n",
    "    elif re == \"Non-Hispanic White\":\n",
    "        return 0.75\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def labelIncome(income): ##the median of range value is taken, all values are calculated as the ratio over 100,000 which is regarded as the max income\n",
    "    if income == '[0,5000)':\n",
    "        return 0.025\n",
    "    elif income == '[5000,10000)':\n",
    "        return 0.075\n",
    "    elif income == '[10000,15000)' or income == '< 20000':\n",
    "        return 0.125\n",
    "    elif income == '[15000,20000)':\n",
    "        return 0.175\n",
    "    elif income == '[20000,25000)':\n",
    "        return 0.225\n",
    "    elif income == '[25000,35000)':\n",
    "        return 0.275\n",
    "    elif income == '[35000,45000)':\n",
    "        return 0.4\n",
    "    elif income == '[45000,55000)' or income == '> 20000':\n",
    "        return 0.5\n",
    "    elif income == '[55000,65000)':\n",
    "        return 0.6\n",
    "    elif income == '[65000,75000)':\n",
    "        return 0.7\n",
    "    elif income == '[75000,100000)':\n",
    "        return 0.875\n",
    "    return 1\n",
    "    \n",
    "pd.options.mode.chained_assignment = None\n",
    "df = df.dropna()\n",
    "df['sex'] = df['sex'].apply(lambda gender: 1 if gender == 'male' else 0 )\n",
    "df['re'] = df['re'].apply(labelRace)\n",
    "df['income'] = df['income'].apply(labelIncome)\n",
    "df['gh'] = df['gh'].apply(lambda value: 1 if value >=6.5 else 0)\n",
    "df['sex'] = df['sex'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13712ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "df_withDiabetes = df[df['gh'] == 1]\n",
    "print(len(df_withDiabetes.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63dab311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>re</th>\n",
       "      <th>income</th>\n",
       "      <th>tx</th>\n",
       "      <th>dx</th>\n",
       "      <th>wt</th>\n",
       "      <th>ht</th>\n",
       "      <th>bmi</th>\n",
       "      <th>leg</th>\n",
       "      <th>arml</th>\n",
       "      <th>armc</th>\n",
       "      <th>waist</th>\n",
       "      <th>tri</th>\n",
       "      <th>sub</th>\n",
       "      <th>albumin</th>\n",
       "      <th>bun</th>\n",
       "      <th>SCr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.973858</td>\n",
       "      <td>-0.444264</td>\n",
       "      <td>0.731330</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>0.646071</td>\n",
       "      <td>-0.252567</td>\n",
       "      <td>0.954476</td>\n",
       "      <td>0.769203</td>\n",
       "      <td>1.178080</td>\n",
       "      <td>1.039658</td>\n",
       "      <td>0.474691</td>\n",
       "      <td>-0.203529</td>\n",
       "      <td>0.630053</td>\n",
       "      <td>1.556518</td>\n",
       "      <td>-1.229289</td>\n",
       "      <td>0.170480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973858</td>\n",
       "      <td>-1.270725</td>\n",
       "      <td>-0.310669</td>\n",
       "      <td>0.122971</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>-0.181390</td>\n",
       "      <td>1.372083</td>\n",
       "      <td>-0.880905</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.995807</td>\n",
       "      <td>-1.078187</td>\n",
       "      <td>-1.234666</td>\n",
       "      <td>-0.977312</td>\n",
       "      <td>-1.113423</td>\n",
       "      <td>0.924423</td>\n",
       "      <td>-0.689919</td>\n",
       "      <td>0.045979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.026651</td>\n",
       "      <td>0.795427</td>\n",
       "      <td>-0.310669</td>\n",
       "      <td>-1.061015</td>\n",
       "      <td>3.543806</td>\n",
       "      <td>2.790407</td>\n",
       "      <td>2.257153</td>\n",
       "      <td>-0.125336</td>\n",
       "      <td>2.780879</td>\n",
       "      <td>-0.843431</td>\n",
       "      <td>0.813535</td>\n",
       "      <td>2.293076</td>\n",
       "      <td>1.658603</td>\n",
       "      <td>1.443882</td>\n",
       "      <td>1.925552</td>\n",
       "      <td>-1.287910</td>\n",
       "      <td>-0.510129</td>\n",
       "      <td>0.593784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.973858</td>\n",
       "      <td>-0.829681</td>\n",
       "      <td>-1.352667</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>1.205018</td>\n",
       "      <td>0.559758</td>\n",
       "      <td>1.024515</td>\n",
       "      <td>0.821223</td>\n",
       "      <td>0.704172</td>\n",
       "      <td>1.169322</td>\n",
       "      <td>0.694180</td>\n",
       "      <td>0.120961</td>\n",
       "      <td>0.424226</td>\n",
       "      <td>-0.339767</td>\n",
       "      <td>-0.869709</td>\n",
       "      <td>-0.178123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.026651</td>\n",
       "      <td>0.294783</td>\n",
       "      <td>0.731330</td>\n",
       "      <td>-0.192759</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>0.607712</td>\n",
       "      <td>0.109553</td>\n",
       "      <td>0.658157</td>\n",
       "      <td>-0.271206</td>\n",
       "      <td>-0.243644</td>\n",
       "      <td>0.369727</td>\n",
       "      <td>0.966879</td>\n",
       "      <td>1.531245</td>\n",
       "      <td>1.005384</td>\n",
       "      <td>-0.023720</td>\n",
       "      <td>0.029242</td>\n",
       "      <td>-0.203023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6788</th>\n",
       "      <td>0.973858</td>\n",
       "      <td>0.417957</td>\n",
       "      <td>-0.310669</td>\n",
       "      <td>-0.903150</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>3.725758</td>\n",
       "      <td>1.146981</td>\n",
       "      <td>3.217276</td>\n",
       "      <td>0.170967</td>\n",
       "      <td>1.250989</td>\n",
       "      <td>3.416831</td>\n",
       "      <td>3.341354</td>\n",
       "      <td>1.993019</td>\n",
       "      <td>0.811665</td>\n",
       "      <td>-0.339767</td>\n",
       "      <td>0.209032</td>\n",
       "      <td>0.344782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6789</th>\n",
       "      <td>0.973858</td>\n",
       "      <td>-0.913122</td>\n",
       "      <td>-0.310669</td>\n",
       "      <td>1.701618</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>-0.027953</td>\n",
       "      <td>0.432526</td>\n",
       "      <td>-0.257737</td>\n",
       "      <td>0.431070</td>\n",
       "      <td>0.995807</td>\n",
       "      <td>0.218453</td>\n",
       "      <td>-0.842245</td>\n",
       "      <td>-1.002273</td>\n",
       "      <td>-1.270820</td>\n",
       "      <td>0.608375</td>\n",
       "      <td>0.388822</td>\n",
       "      <td>0.618684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6791</th>\n",
       "      <td>-1.026651</td>\n",
       "      <td>0.259022</td>\n",
       "      <td>0.731330</td>\n",
       "      <td>-1.376744</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>2.790407</td>\n",
       "      <td>0.629631</td>\n",
       "      <td>-1.015957</td>\n",
       "      <td>1.521972</td>\n",
       "      <td>-1.207575</td>\n",
       "      <td>-0.826915</td>\n",
       "      <td>1.169322</td>\n",
       "      <td>0.408179</td>\n",
       "      <td>1.319078</td>\n",
       "      <td>0.690590</td>\n",
       "      <td>-0.655815</td>\n",
       "      <td>-1.049499</td>\n",
       "      <td>0.045979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6792</th>\n",
       "      <td>0.973858</td>\n",
       "      <td>-0.762134</td>\n",
       "      <td>-1.352667</td>\n",
       "      <td>-0.192759</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>-1.019810</td>\n",
       "      <td>-0.291716</td>\n",
       "      <td>-1.038942</td>\n",
       "      <td>-0.843431</td>\n",
       "      <td>-1.118550</td>\n",
       "      <td>-0.429867</td>\n",
       "      <td>-1.334433</td>\n",
       "      <td>-1.726135</td>\n",
       "      <td>-1.561399</td>\n",
       "      <td>0.608375</td>\n",
       "      <td>-0.330339</td>\n",
       "      <td>0.170480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6793</th>\n",
       "      <td>0.973858</td>\n",
       "      <td>1.538447</td>\n",
       "      <td>-0.310669</td>\n",
       "      <td>-1.061015</td>\n",
       "      <td>-0.282130</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>-0.027953</td>\n",
       "      <td>-0.448308</td>\n",
       "      <td>0.263066</td>\n",
       "      <td>0.014906</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>-0.084097</td>\n",
       "      <td>0.714134</td>\n",
       "      <td>0.220804</td>\n",
       "      <td>0.169969</td>\n",
       "      <td>-0.971863</td>\n",
       "      <td>1.107982</td>\n",
       "      <td>1.166490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5343 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sex       age        re    income        tx        dx        wt  \\\n",
       "0     0.973858 -0.444264  0.731330 -0.587421 -0.282130 -0.358304  0.646071   \n",
       "1     0.973858 -1.270725 -0.310669  0.122971 -0.282130 -0.358304 -0.181390   \n",
       "2    -1.026651  0.795427 -0.310669 -1.061015  3.543806  2.790407  2.257153   \n",
       "3     0.973858 -0.829681 -1.352667 -0.587421 -0.282130 -0.358304  1.205018   \n",
       "4    -1.026651  0.294783  0.731330 -0.192759 -0.282130 -0.358304  0.607712   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6788  0.973858  0.417957 -0.310669 -0.903150 -0.282130 -0.358304  3.725758   \n",
       "6789  0.973858 -0.913122 -0.310669  1.701618 -0.282130 -0.358304 -0.027953   \n",
       "6791 -1.026651  0.259022  0.731330 -1.376744 -0.282130  2.790407  0.629631   \n",
       "6792  0.973858 -0.762134 -1.352667 -0.192759 -0.282130 -0.358304 -1.019810   \n",
       "6793  0.973858  1.538447 -0.310669 -1.061015 -0.282130 -0.358304 -0.027953   \n",
       "\n",
       "            ht       bmi       leg      arml      armc     waist       tri  \\\n",
       "0    -0.252567  0.954476  0.769203  1.178080  1.039658  0.474691 -0.203529   \n",
       "1     1.372083 -0.880905  0.899254  0.995807 -1.078187 -1.234666 -0.977312   \n",
       "2    -0.125336  2.780879 -0.843431  0.813535  2.293076  1.658603  1.443882   \n",
       "3     0.559758  1.024515  0.821223  0.704172  1.169322  0.694180  0.120961   \n",
       "4     0.109553  0.658157 -0.271206 -0.243644  0.369727  0.966879  1.531245   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "6788  1.146981  3.217276  0.170967  1.250989  3.416831  3.341354  1.993019   \n",
       "6789  0.432526 -0.257737  0.431070  0.995807  0.218453 -0.842245 -1.002273   \n",
       "6791 -1.015957  1.521972 -1.207575 -0.826915  1.169322  0.408179  1.319078   \n",
       "6792 -0.291716 -1.038942 -0.843431 -1.118550 -0.429867 -1.334433 -1.726135   \n",
       "6793 -0.448308  0.263066  0.014906  0.011537 -0.084097  0.714134  0.220804   \n",
       "\n",
       "           sub   albumin       bun       SCr  \n",
       "0     0.630053  1.556518 -1.229289  0.170480  \n",
       "1    -1.113423  0.924423 -0.689919  0.045979  \n",
       "2     1.925552 -1.287910 -0.510129  0.593784  \n",
       "3     0.424226 -0.339767 -0.869709 -0.178123  \n",
       "4     1.005384 -0.023720  0.029242 -0.203023  \n",
       "...        ...       ...       ...       ...  \n",
       "6788  0.811665 -0.339767  0.209032  0.344782  \n",
       "6789 -1.270820  0.608375  0.388822  0.618684  \n",
       "6791  0.690590 -0.655815 -1.049499  0.045979  \n",
       "6792 -1.561399  0.608375 -0.330339  0.170480  \n",
       "6793  0.169969 -0.971863  1.107982  1.166490  \n",
       "\n",
       "[5343 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressors = df[['sex','age','re','income','tx','dx','wt','ht','bmi','leg','arml','armc','waist','tri','sub','albumin','bun','SCr']]\n",
    "normalized_regressors = (regressors - regressors.mean())/regressors.std()\n",
    "regressand = df['gh']\n",
    "normalized_regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e256c",
   "metadata": {},
   "source": [
    "<font color='blue' size=\"+2\"><b>Logistic Regression:<b><font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da793631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.136025\n",
      "         Iterations 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy =  0.9631940112289458\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm  ##statsmodel package used as the model summary contains more statistical info\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(regressors,regressand,test_size=0.3,random_state=0)\n",
    "\n",
    "vanilla_model = sm.Logit(y_train,x_train).fit()\n",
    "predictY = vanilla_model.predict(x_test)\n",
    "predictY = list(map(round, predictY))\n",
    "cm = confusion_matrix(y_test, predictY)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "plt.show()\n",
    "print('Test accuracy = ', accuracy_score(y_test, predictY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018adaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                     gh   No. Observations:                 3740\n",
      "Model:                          Logit   Df Residuals:                     3722\n",
      "Method:                           MLE   Df Model:                           17\n",
      "Date:                Sat, 30 Dec 2023   Pseudo R-squ.:                  0.4938\n",
      "Time:                        18:20:22   Log-Likelihood:                -508.73\n",
      "converged:                       True   LL-Null:                       -1005.0\n",
      "Covariance Type:            nonrobust   LLR p-value:                3.402e-200\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "sex            0.3660      0.300      1.221      0.222      -0.221       0.953\n",
      "age            0.0209      0.007      2.994      0.003       0.007       0.035\n",
      "re            -0.9546      0.385     -2.477      0.013      -1.710      -0.199\n",
      "income        -0.6564      0.316     -2.079      0.038      -1.275      -0.038\n",
      "tx             2.2315      0.258      8.664      0.000       1.727       2.736\n",
      "dx             1.8345      0.252      7.288      0.000       1.341       2.328\n",
      "wt             0.0189      0.014      1.310      0.190      -0.009       0.047\n",
      "ht            -0.0196      0.017     -1.123      0.262      -0.054       0.015\n",
      "bmi           -0.1474      0.057     -2.594      0.009      -0.259      -0.036\n",
      "leg           -0.0288      0.036     -0.810      0.418      -0.099       0.041\n",
      "arml          -0.0275      0.062     -0.444      0.657      -0.149       0.094\n",
      "armc           0.0583      0.047      1.235      0.217      -0.034       0.151\n",
      "waist          0.0480      0.018      2.664      0.008       0.013       0.083\n",
      "tri           -0.0254      0.020     -1.296      0.195      -0.064       0.013\n",
      "sub            0.0452      0.016      2.764      0.006       0.013       0.077\n",
      "albumin       -0.7092      0.293     -2.424      0.015      -1.283      -0.136\n",
      "bun            0.0110      0.016      0.681      0.496      -0.021       0.042\n",
      "SCr           -0.5392      0.324     -1.666      0.096      -1.174       0.095\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(vanilla_model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75493e",
   "metadata": {},
   "source": [
    "<font color='blue'><b>From the confusion matrix, it can be seen that the model's performance is not very good, the false negative rate is 33% which is quite high. From the summary of the model, it can be seen that regressors (sex, wt, ht, leg, arml, armc, tri, bun) have very high p-values, meaning there is a decent chance they have little impact on the model. Therefore, a new list of regressors is created with these 8 columns removed.<b><font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb86ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.138920\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy =  0.9631940112289458\n"
     ]
    }
   ],
   "source": [
    "new_regressor = df[['age','re','income','tx','dx','bmi','waist','sub','albumin','SCr']]\n",
    "x_train,x_test, y_train, y_test = train_test_split(new_regressor,regressand,test_size=0.3,random_state=0)\n",
    "\n",
    "new_model = sm.Logit(y_train,x_train).fit()\n",
    "predictY = new_model.predict(x_test)\n",
    "predictY = list(map(round, predictY))\n",
    "cm = confusion_matrix(y_test, predictY)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(cm)\n",
    "ax.grid(False)\n",
    "ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "ax.set_ylim(1.5, -0.5)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "plt.show()\n",
    "print('Test accuracy = ', accuracy_score(y_test, predictY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb75630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                     gh   No. Observations:                 3740\n",
      "Model:                          Logit   Df Residuals:                     3730\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Sat, 30 Dec 2023   Pseudo R-squ.:                  0.4830\n",
      "Time:                        18:20:22   Log-Likelihood:                -519.56\n",
      "converged:                       True   LL-Null:                       -1005.0\n",
      "Covariance Type:            nonrobust   LLR p-value:                3.125e-203\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "age            0.0166      0.006      2.939      0.003       0.006       0.028\n",
      "re            -1.2481      0.364     -3.427      0.001      -1.962      -0.534\n",
      "income        -0.5863      0.305     -1.925      0.054      -1.183       0.011\n",
      "tx             2.2951      0.254      9.050      0.000       1.798       2.792\n",
      "dx             1.8686      0.250      7.476      0.000       1.379       2.358\n",
      "bmi           -0.0991      0.034     -2.919      0.004      -0.166      -0.033\n",
      "waist          0.0493      0.013      3.757      0.000       0.024       0.075\n",
      "sub            0.0309      0.014      2.143      0.032       0.003       0.059\n",
      "albumin       -1.4307      0.136    -10.498      0.000      -1.698      -1.164\n",
      "SCr           -0.4168      0.209     -1.996      0.046      -0.826      -0.007\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(new_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3650c74",
   "metadata": {},
   "source": [
    "<font color='blue'><b>From the result it can be seen that the new model has about the same performance as the old one and removing the 9 columns is a correct decision. So the fine-tuning will be based on this new model. For tuning hyperparameters, the sklearn model library is used instead since it supports custom input for hyperparameters like solver, etc which is not supported in statsmodel package.<font><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a54a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619463505926388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1478,   25],\n",
       "       [  36,   64]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model1 = LogisticRegression(solver='newton-cg', random_state=0).fit(x_train, y_train)\n",
    "print(model1.score(x_test,y_test))\n",
    "confusion_matrix(y_test, model1.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be6820b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9631940112289458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1479,   24],\n",
       "       [  35,   65]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LogisticRegression(solver='liblinear', random_state=0, penalty = 'l1').fit(x_train, y_train)\n",
    "print(model2.score(x_test,y_test))\n",
    "confusion_matrix(y_test, model2.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aafe07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9631940112289458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1479,   24],\n",
       "       [  35,   65]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = LogisticRegression(solver='liblinear', random_state=0, penalty = 'l2').fit(x_train, y_train)\n",
    "print(model3.score(x_test,y_test))\n",
    "confusion_matrix(y_test, model3.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd9d2f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9631940112289458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1479,   24],\n",
       "       [  35,   65]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = LogisticRegression(solver='liblinear', random_state=0, penalty = 'l2',C=3).fit(x_train, y_train)\n",
    "print(model3.score(x_test,y_test))\n",
    "confusion_matrix(y_test, model3.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016bb03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9625701809107923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1479,   24],\n",
       "       [  36,   64]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = LogisticRegression(solver='liblinear', random_state=0, penalty = 'l2',C=0.5).fit(x_train, y_train)\n",
    "print(model3.score(x_test,y_test))\n",
    "confusion_matrix(y_test, model3.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3cc28",
   "metadata": {},
   "source": [
    "<font color='blue'><b>As shown above, the solver of liblinear, default penalty of L2 and default regularization strength of 1 will produce a \n",
    "decent result already.<font color><b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82145f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
